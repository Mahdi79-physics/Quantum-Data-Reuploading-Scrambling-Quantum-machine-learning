## ğŸ§¬ Data Re-Uploading Quantum Classifier with Scrambling Ansatz  
**SMOTEENN-Balanced Quantum Machine Learning for Materials Data**

This repository implements a **variational quantum machine learning (QML) classifier** based on **data re-uploading**, **scrambling-inspired entangling layers**, and **margin-based optimization**, applied to materials classification data.

The model combines **classical data balancing**, **feature engineering from atomic properties**, and a **multi-layer quantum circuit** trained using **adjoint differentiation** in PennyLane.

---

## ğŸ§  Core Idea

This project explores how **data re-uploading quantum circuits** enhance expressive power in variational quantum classifiers by repeatedly encoding classical data between entangling layers.

**Key ingredients:**

- ğŸ” **Data re-uploading** for increased nonlinearity  
- ğŸ§© **Scrambling entangling ansatz** for strong qubit mixing  
- âš–ï¸ **SMOTEENN resampling** to handle class imbalance  
- ğŸ“ **Margin (hinge) loss** for robust classification  
- âš¡ **Adjoint differentiation** for efficient gradient evaluation  

---

## ğŸ“‚ Repository Structure

```text
Data-Reuploading-QML/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ TableS1.csv
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ data_reuploading_scrambling.py
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ training_loss.png
â”‚   â””â”€â”€ accuracy.png
â”‚
â””â”€â”€ results/
    â””â”€â”€ SMOTE_Scrambling_Results.csv
---

## ğŸ“Š Dataset & Feature Engineering

The dataset used in this project is loaded from `TableS1.csv` and contains experimentally and computationally derived materials descriptors. Prior to training, the data undergoes **feature selection, physical normalization, and class balancing** to ensure stable quantum training.

---

### ğŸ” Selected Features

Five physically motivated features are extracted and used as inputs to the quantum classifier:

| Feature | Description |
|--------|-------------|
| Ï„ | Structural tolerance parameter |
| t | Goldschmidt tolerance factor |
| r<sub>A</sub> / r<sub>X</sub> | Ionic radius ratio between A-site cation and halide |
| r<sub>B</sub> / r<sub>X</sub> | Ionic radius ratio between B-site cation and halide |
| Î”Ï‡(Bâ€“X) | Electronegativity difference between B-site cation and halide |

These features are chosen to capture **geometric stability**, **ionic size mismatch**, and **bond polarity**, which are critical for materials classification tasks.

---

### ğŸ§ª Elemental Property Extraction

Element-specific quantities are automatically retrieved using the **Mendeleev** library, including:

- Pauling electronegativity  
- Ionic and atomic radii  

This allows the feature construction pipeline to remain **generalizable** to unseen chemical compositions.

---

### âš™ï¸ Preprocessing Pipeline

The preprocessing workflow consists of:

1. Removal of incomplete or non-physical entries  
2. Feature normalization to zero mean and unit variance  
3. Scaling of all features to the interval  
   \[
   [0, \pi]
   \]
   for quantum angle encoding  
4. Stratified trainâ€“test split to preserve class ratios  

---

### âš–ï¸ Class Imbalance Handling

To mitigate strong class imbalance in the dataset, the training set is resampled using **SMOTEENN**, which combines:

- **SMOTE** â€” Synthetic Minority Over-sampling  
- **ENN** â€” Edited Nearest Neighbors cleaning  

> **Important:** Resampling is applied **only to the training set** to prevent information leakage into the test data.

---

### ğŸ¯ Final Input Representation

Each material sample is represented as a **5-dimensional feature vector**:

\[
\mathbf{x} = (x_1, x_2, x_3, x_4, x_5)
\]

These vectors are used directly as inputs to the **data re-uploading quantum circuit**.

---
## ğŸ”€ Quantum Circuit Architecture

The quantum model is implemented as a **variational data re-uploading circuit** with strongly entangling scrambling layers, designed to maximize expressivity under a limited qubit budget.

---

### ğŸ–¥ï¸ Hardware Setup

- **Number of qubits:** 5  
- **Quantum framework:** PennyLane  
- **Device:** `default.qubit` (statevector simulator)  
- **Differentiation method:** Adjoint differentiation  

---

### ğŸ§© Overall Circuit Structure

The circuit follows a **layered re-uploading architecture**:


Each classical input vector is **re-encoded at every layer**, allowing the variational circuit to build high-order nonlinear decision boundaries.

---

### ğŸ” Data Re-Uploading Layers

At each layer, the classical feature vector  
\[
\mathbf{x} \in \mathbb{R}^5
\]
is encoded using **angle encoding**:

- Global Hadamard initialization
- Feature-dependent phase rotations:
  \[
  RZ(2x_i)
  \]
  applied to qubit *i*

This repeated encoding significantly increases the expressive power of shallow quantum circuits.

---

### ğŸ§± Scrambling Entangling Ansatz

Between data encoding stages, a **scrambling entangling layer** is applied to promote strong qubit mixing and correlation.

#### ğŸ”— Entanglement Layout (Brick-Wall)

Entangling blocks are applied sequentially on the following qubit pairs:

- (0, 1)
- (2, 3)
- (1, 2)
- (3, 4)
- (4, 0)

This layout ensures **full connectivity** across the register over a single layer.

---

### ğŸ”§ Entangling Block Structure

Each two-qubit block consists of:

1. Local rotations:  
   \[
   RY(\theta_1) \otimes RY(\theta_2)
   \]
2. Controlled-NOT (CNOT)
3. Local rotations:  
   \[
   RY(\theta_3) \otimes RY(\theta_4)
   \]
4. Reverse CNOT

- **Parameters per block:** 4  
- **Blocks per layer:** 5  
- **Total parameters per layer:** 20  

---

### ğŸ”„ Layer Repetition

The full variational circuit applies multiple repetitions of:


This **Encodeâ€“Entangleâ€“Encode** pattern allows the model to represent complex, highly nonlinear decision functions.

---

### ğŸ“ Parameter Scaling

Let:
- \( L \) = number of re-uploading layers  

Then the total number of trainable parameters is:

\[
N_{\text{params}} = 20 \times L
\]

This linear scaling enables controlled expressivity without excessive parameter growth.

---

### ğŸ§  Design Motivation

This architecture is chosen to:

- Avoid barren plateaus via shallow but expressive layers  
- Maximize entanglement efficiency  
- Support structured inductive bias for scientific data  
- Remain compatible with near-term quantum hardware  

---

### ğŸ§ª Summary

| Component | Choice |
|---------|-------|
| Encoding | Angle encoding (RZ) |
| Entanglement | Scrambling brick-wall |
| Expressivity | Data re-uploading |
| Readout | Single-qubit expectation |
| Scalability | Linear in layer count |
---
## ğŸ“ Measurement & Output

The quantum classifier produces a **single scalar output** obtained from a projective measurement on one qubit.
---
### ğŸ” Observable

The model measures the expectation value of the Pauli-Z operator on qubit 0:

\[
\langle Z_0 \rangle
\]

This value lies in the interval:

\[
\langle Z_0 \rangle \in [-1, 1]
\]

---

### ğŸ§® Classification Rule

The expectation value is mapped to a binary class label using a threshold decision rule:

- **\(\langle Z_0 \rangle \ge 0\)** â†’ Class **+1**
- **\(\langle Z_0 \rangle < 0\)** â†’ Class **âˆ’1**

This simple readout:
- Minimizes measurement overhead
- Is compatible with near-term hardware
- Aligns naturally with margin-based losses

---

### ğŸ“¤ Output Interpretation

The output expectation value can be interpreted as:
- A signed confidence score
- A soft decision boundary indicator
- A margin proxy for classification robustness

---

## ğŸ¯ Loss Function

Training is performed using a **margin-based (hinge) loss**, commonly used in large-margin classifiers.

### ğŸ“‰ Loss Definition

For a true label \( y \in \{-1, +1\} \) and model prediction \( \hat{y} = \langle Z_0 \rangle \), the loss is:

\[
\mathcal{L}
=
\mathbb{E}
\left[
\max\left(0,\; 1 - y \cdot \hat{y}\right)^2
\right]
\]

---

### âœ… Why Margin Loss?

The squared hinge loss is chosen because it:

- Encourages **confident predictions**
- Penalizes misclassified samples strongly
- Is more stable than mean-squared error (MSE)
- Works naturally with expectation-value outputs

---

### ğŸ§  Optimization Objective

The training objective is to **maximize the classification margin** while minimizing misclassification error across the dataset.

---

## âš™ï¸ Training Details

### ğŸ§ª Optimization Setup

- **Optimizer:** Adam  
- **Learning rate:** 0.02  
- **Epochs:** 30  
- **Batch size:** 64  

---

### âš¡ Differentiation Strategy

Gradients are computed using:

- **Adjoint differentiation**

This method:
- Computes exact gradients
- Scales efficiently with circuit depth
- Avoids sampling noise from finite shots

---

### ğŸ“Š Metrics Tracked

During training, the following metrics are recorded:

- Training loss
- Training accuracy
- Test accuracy

These metrics are used to evaluate convergence, generalization, and overfitting behavior.

---

### ğŸ” Training Workflow

1. Initialize circuit parameters randomly  
2. Encode classical data into the quantum circuit  
3. Measure expectation values  
4. Compute hinge loss  
5. Update parameters via gradient descent  
6. Repeat for all epochs  

---

### ğŸ§  Stability Considerations

To ensure reliable training:

- Data re-uploading mitigates barren plateaus  
- Strong entangling layers improve gradient flow  
- Margin loss prevents vanishing gradients  

---

### ğŸ“Œ Summary

| Component | Choice |
|--------|--------|
| Observable | \( \langle Z_0 \rangle \) |
| Output type | Expectation value |
| Loss | Squared hinge loss |
| Optimizer | Adam |
| Gradients | Adjoint method |
| Target task | Binary classification |

## ğŸ“ˆ Results & Outputs

The training and evaluation process produces both **visual diagnostics** and **structured numerical outputs** to assess model performance and stability.

---

### ğŸ“‰ Training Loss

- The loss decreases steadily across epochs, indicating stable optimization.
- Margin-based loss ensures confident separation between classes.
- No abrupt oscillations are observed, reflecting good gradient behavior.

**Figure:**
- `figures/training_loss.png`

---

### ğŸ“ˆ Classification Accuracy

Accuracy is tracked throughout training for both training and test sets.

- Training accuracy improves monotonically.
- Test accuracy follows closely, indicating good generalization.
- No severe overfitting is observed despite strong circuit expressivity.

**Figures:**
- `figures/accuracy.png`

---

### ğŸ“ Prediction Outputs

Final predictions are saved in a structured CSV file:


Each row contains:
- Encoded feature values
- Ground-truth class label
- Predicted expectation value âŸ¨Zâ‚€âŸ©
- Final predicted class (+1 / âˆ’1)

This format enables:
- Post-hoc statistical analysis
- Benchmarking against classical models
- Reproducible evaluation pipelines

---

### ğŸ“Š Summary of Outputs

| Output Type | Description |
|-----------|------------|
| Training loss | Optimization convergence |
| Accuracy curves | Generalization performance |
| CSV results | Sample-level predictions |
| Figures | Visual diagnostics |

---

## ğŸ§ª Why This Model Matters

This project demonstrates a **practical, research-grade quantum machine learning pipeline** rather than a toy example.

### ğŸ”¬ Scientific Relevance

- Applies QML to **real materials data**
- Handles **severe class imbalance** correctly
- Uses domain-aware feature engineering

---

### âš›ï¸ Quantum Machine Learning Contributions

- Demonstrates **data re-uploading** as a solution to limited circuit expressivity
- Introduces **scrambling entangling layers** for strong qubit mixing
- Uses **adjoint differentiation**, enabling scalable training

---

### ğŸ“ Methodological Strengths

- Margin-based loss improves robustness
- Single-observable readout minimizes measurement cost
- Compatible with near-term quantum hardware

---

### ğŸ§  Broader Impact

This model serves as:
- A benchmark variational quantum classifier
- A template for scientific QML workflows
- A bridge between quantum algorithms and materials informatics

---

## ğŸš€ Future Extensions

Several natural extensions can further enhance this work:

---

### âš™ï¸ Quantum Enhancements

- Add realistic noise models using `default.mixed`
- Shot-based training to mimic hardware conditions
- Multi-qubit observables for richer readout

---

### ğŸ“Š Model Improvements

- Hyperparameter optimization (depth, learning rate)
- Alternative loss functions (logistic, focal loss)
- Adaptive data re-uploading strategies

---

### ğŸ§ª Benchmarking & Validation

- Compare against classical ML baselines (SVM, XGBoost, NN)
- Perform cross-validation and statistical tests
- Evaluate robustness under noisy labels

---

### ğŸ–¥ï¸ Hardware Execution

- Deploy on cloud quantum hardware
- Analyze hardware-induced bias and noise
- Compare simulation vs hardware performance

---

### ğŸ“Œ Outlook

This repository provides a strong foundation for:
- Research-grade QML experimentation
- Materials discovery pipelines
- Hybrid quantumâ€“classical learning systems

