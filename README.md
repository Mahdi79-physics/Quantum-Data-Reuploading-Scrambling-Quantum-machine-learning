##ğŸ§¬ Data Re-Uploading Quantum Classifier with Scrambling Ansatz
SMOTEENN-Balanced Quantum Machine Learning for Materials Data

This repository implements a variational quantum machine learning (QML) classifier based on data re-uploading, entangling scrambling layers, and margin-based optimization, applied to materials classification data.

The model combines classical data balancing, feature engineering from atomic properties, and a multi-layer quantum circuit trained using adjoint differentiation in PennyLane.

##ğŸ§  Core Idea

This project explores how data re-uploading quantum circuits can enhance expressive power in variational quantum classifiers by repeatedly encoding classical data between entangling layers.

Key ingredients:

#ğŸ” Data re-uploading for increased nonlinearity

#ğŸ§© Scrambling entangling ansatz for strong qubit mixing

#âš–ï¸ SMOTEENN resampling to handle class imbalance

#ğŸ“ Margin (hinge) loss for robust classification

#âš¡ Adjoint differentiation for efficient gradient evaluation

#ğŸ“‚ Repository Structure
Data-Reuploading-QML/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ TableS1.csv
â”‚
â”œâ”€â”€ model/
â”‚   â””â”€â”€ data_reuploading_scrambling.py
â”‚
â”œâ”€â”€ figures/
â”‚   â”œâ”€â”€ training_loss.png
â”‚   â”œâ”€â”€ accuracy.png
â”‚
â””â”€â”€ results/
    â””â”€â”€ SMOTE_Scrambling_Results.csv

##ğŸ“Š Dataset & Feature Engineering

The dataset is loaded from TableS1.csv and processed using domain-aware feature construction:

Selected Features (5)
Feature	Description
Ï„	Structural tolerance parameter
t	Goldschmidt tolerance factor
rA / rX	Ionic radius ratio
rB / rX	Ionic radius ratio
Î”Ï‡(Bâ€“X)	Electronegativity difference

Elemental properties are extracted automatically using Mendeleev:

electronegativity

atomic number (if needed for extensions)

##âš–ï¸ Class Imbalance Handling

To address strong label imbalance, the training data is balanced using:

SMOTEENN

Synthetic Minority Over-sampling (SMOTE)

Edited Nearest Neighbors (ENN) cleaning

##âœ… Resampling is applied only to the training set to avoid data leakage.

##ğŸ”¢ Data Encoding

Each classical feature vector

ğ‘¥
âˆˆ
ğ‘…
5
xâˆˆR
5

is scaled to 
[
0
,
ğœ‹
]
[0,Ï€] and encoded using angle encoding:

Hadamard initialization

Phase encoding via RZ(2x_i)

This encoding is repeated at every layer (data re-uploading).

##ğŸ”€ Quantum Circuit Architecture
Hardware

5 qubits

PennyLane default.qubit device

Ansatz Structure

Each layer consists of:

Data Encoding

Scrambling Entangling Layer

Scrambling Layout (Brick-Wall)

Entangling blocks act on:

(0,1), (2,3), (1,2), (3,4), (4,0)


Each block:

RY âŠ— RY â†’ CNOT â†’ RY âŠ— RY â†’ CNOT (reversed)


4 parameters per block

20 parameters per layer

Data Re-Uploading

The full circuit applies:

Encode â†’ Scramble â†’ Encode â†’ Scramble â†’ ...


for multiple layers.

##ğŸ“ Measurement & Output

The model outputs a single expectation value:

âŸ¨
ğ‘
0
âŸ©
âŸ¨Z
0
	â€‹

âŸ©

Classification rule:

â‰¥ 0 â†’ class +1

< 0 â†’ class âˆ’1

##ğŸ¯ Loss Function

A margin-based (hinge) loss is used:

ğ¿
=
ğ¸
[
max
â¡
(
0
,
1
âˆ’
ğ‘¦
â‹…
ğ‘¦
^
)
2
]
L=E[max(0,1âˆ’yâ‹…
y
^
	â€‹

)
2
]

Why margin loss?

More stable than MSE

Encourages confident classification

Common in large-margin classifiers

##âš™ï¸ Training Details

Optimizer: Adam

Learning rate: 0.02

Epochs: 30

Batch size: 64

Differentiation: adjoint method

Training metrics tracked:

Training loss

Training accuracy

Test accuracy

##ğŸ“ˆ Results & Outputs

During training, the following are produced:

##ğŸ“‰ Loss vs Epoch

##ğŸ“ˆ Train/Test Accuracy vs Epoch

##ğŸ“ CSV file with predictions:

SMOTE_Scrambling_Results.csv


Each row contains:

Encoded test features

Ground-truth label

Model prediction

##ğŸ§ª Why This Model Matters

This implementation demonstrates:

Practical data re-uploading in QML

Strong entangling expressivity via scrambling

Correct handling of imbalanced scientific datasets

Scalable gradient evaluation using adjoint differentiation

It is suitable as:

A QML research prototype

A benchmark variational classifier

A base model for materials informatics

##ğŸš€ Future Extensions

Noise models (default.mixed)

Shot-based training

Multi-observable readout

Comparison with classical baselines

Hardware execution

##ğŸ‘¤ Author

Mahdi
Quantum Machine Learning â€¢ Variational Circuits â€¢ Materials Informatics
